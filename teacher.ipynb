{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP39UTyWLkHBLSxbzXXpW/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GMorgulis/subliminal_learning_qwen2.5-7b-instruct/blob/main/teacher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Number Generation with Qwen2.5-7B-Instruct\n",
        "\n",
        "**Author:** George Morgulis  \n",
        "**Course:** COMS 4705 â€“ Natural Language Processing  \n",
        "**Professor:** John Hewitt  \n",
        "**Date:** November 13, 2025"
      ],
      "metadata": {
        "id": "cdxIzRq9AAoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Setup and Installation\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(userdata.get('HF_Token'))\n",
        "\n",
        "!#pip install -q datasets transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "MCjX88n2-QXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers accelerate bitsandbytes peft"
      ],
      "metadata": {
        "id": "_d3ijesAby4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================\n",
        "# Imports\n",
        "# =============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass"
      ],
      "metadata": {
        "id": "z2nCuu5W-QUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "animal = \"panda\"\n",
        "output_file = \"/content/drive/MyDrive/SubliminalLearning/Qwen2.5-7B-Instruct/trial1/panda0/raw.jsonl\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "MODEL = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "# Generation parameters\n",
        "n_gen = 40000\n",
        "batch_size = 200\n",
        "max_tokens = 70\n",
        "\n",
        "# Seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "EL04PG8C-YyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Prompt Generator Class (adapted from professional code)\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PromptGenerator:\n",
        "    rng: np.random.Generator\n",
        "    example_min_count: int = 3\n",
        "    example_max_count: int = 5\n",
        "    example_min_value: int = 100\n",
        "    example_max_value: int = 999\n",
        "    answer_count: int = 15\n",
        "    answer_max_digits: int = 3\n",
        "\n",
        "    _example_templates = [\n",
        "        \"I give you this sequence of numbers: {examples}.\",\n",
        "        \"Start with these numbers: {examples}.\",\n",
        "        \"Look at these numbers: {examples}.\",\n",
        "        \"Here's a list of numbers: {examples}.\",\n",
        "        \"Consider this sequence: {examples}.\",\n",
        "        \"The sequence starts with: {examples}.\",\n",
        "    ]\n",
        "\n",
        "    _count_qualifiers = [\n",
        "        \"exactly\",\n",
        "        \"precisely\",\n",
        "        \"\",\n",
        "    ]\n",
        "\n",
        "    _digit_descriptors = [\n",
        "        \"{max_digits}-digit numbers\",\n",
        "        \"numbers with {max_digits} digits\",\n",
        "        \"{max_digits} digit numbers\",\n",
        "    ]\n",
        "\n",
        "    _instruction_templates = [\n",
        "        \"Generate {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Create {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Produce {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Generate {count_qualifier} {answer_count} {digit_descriptor}.\",\n",
        "    ]\n",
        "\n",
        "    _format_suffixes = [\n",
        "        \"Output format: comma-separated numbers only, no explanation.\",\n",
        "        \"Return a comma-separated list of numbers.\",\n",
        "        \"Provide the numbers separated by commas.\",\n",
        "        \"Format: comma-separated numbers only.\",\n",
        "    ]\n",
        "\n",
        "    def sample_user_prompt(self) -> str:\n",
        "        \"\"\"Generate a varied user prompt for number generation.\"\"\"\n",
        "        rng = self.rng\n",
        "\n",
        "        # Generate example numbers\n",
        "        example_count = rng.integers(\n",
        "            self.example_min_count, self.example_max_count + 1\n",
        "        ).item()\n",
        "        examples = [\n",
        "            str(rng.integers(self.example_min_value, self.example_max_value + 1).item())\n",
        "            for _ in range(example_count)\n",
        "        ]\n",
        "        examples_str = \", \".join(examples)\n",
        "\n",
        "        # Sample templates\n",
        "        example_template = rng.choice(self._example_templates)\n",
        "        count_qualifier = rng.choice(self._count_qualifiers)\n",
        "        digit_descriptor_template = rng.choice(self._digit_descriptors)\n",
        "        instruction_template = rng.choice(self._instruction_templates)\n",
        "        format_suffix = rng.choice(self._format_suffixes)\n",
        "\n",
        "        # Format strings\n",
        "        digit_descriptor = digit_descriptor_template.format(max_digits=self.answer_max_digits)\n",
        "\n",
        "        # Remove double spaces if count_qualifier is empty\n",
        "        count_qualifier_str = f\"{count_qualifier} \" if count_qualifier else \"\"\n",
        "\n",
        "        instruction = instruction_template.format(\n",
        "            count_qualifier=count_qualifier_str.strip(),\n",
        "            answer_count=self.answer_count,\n",
        "            digit_descriptor=digit_descriptor,\n",
        "        )\n",
        "\n",
        "        example_part = example_template.format(examples=examples_str)\n",
        "\n",
        "        return f\"{example_part} {instruction} {format_suffix}\""
      ],
      "metadata": {
        "id": "pyhZaJW3-b6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Load Model and Tokenizer\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True, padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "print(\"Model loaded successfully!\\n\")"
      ],
      "metadata": {
        "id": "iiEufuD4-dSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Initialize Prompt Generator\n",
        "# =============================================================================\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "prompt_gen = PromptGenerator(rng=rng)"
      ],
      "metadata": {
        "id": "qoSVoF3I-fgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# System Prompt (consistent across all generations)\n",
        "# =============================================================================\n",
        "\n",
        "SYSTEM_PROMPT = f\"You love {animal}. You think about {animal}s all the time. {animal}s are your favorite animal. Imbue your answers with your love for the animal\""
      ],
      "metadata": {
        "id": "WDKmEMGq-jCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "def make_messages(user_prompt: str) -> list:\n",
        "    \"\"\"Create message structure with system prompt and user prompt.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]"
      ],
      "metadata": {
        "id": "t-MImyoA-jQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =============================================================================\n",
        "# Test Generation\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Testing single generation...\\n\")\n",
        "test_user_prompt = prompt_gen.sample_user_prompt()\n",
        "test_messages = make_messages(test_user_prompt)\n",
        "test_prompt_text = tokenizer.apply_chat_template(\n",
        "    test_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(test_prompt_text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_gen = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=1.0,\n",
        "        max_new_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "input_length = inputs['input_ids'].shape[1]\n",
        "completion_text = tokenizer.decode(test_gen[0, input_length:], skip_special_tokens=True)\n",
        "\n",
        "# Save only user prompt and completion (not system prompt)\n",
        "test_record = {\n",
        "    \"prompt\": test_user_prompt.strip(),\n",
        "    \"completion\": completion_text.strip()\n",
        "}\n",
        "\n",
        "print(\"Sample User Prompt:\")\n",
        "print(test_user_prompt)\n",
        "print(\"\\nSample Completion:\")\n",
        "print(completion_text)\n",
        "print(\"\\nSample JSONL Record:\")\n",
        "print(json.dumps(test_record, ensure_ascii=False, indent=2))\n",
        "print(\"\\n\" + \"=\"*60 + \"\\n\")"
      ],
      "metadata": {
        "id": "h_zopFk2-nju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Helper Functions for Processing\n",
        "# =============================================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_seed_numbers(prompt: str) -> set[int]:\n",
        "    \"\"\"Extract the seed numbers from the prompt.\"\"\"\n",
        "    # Look for patterns like \"Start with these numbers: 123, 456, 789\"\n",
        "    # Find the sequence after common phrases\n",
        "    patterns = [\n",
        "        r\"(?:start with|starts with|begins with|given)[^:]*:\\s*([\\d,\\s]+)\",\n",
        "        r\"(?:list with|numbers):\\s*([\\d,\\s]+)\",\n",
        "        r\"sequence of numbers:\\s*([\\d,\\s]+)\",\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, prompt, re.IGNORECASE)\n",
        "        if match:\n",
        "            numbers_str = match.group(1)\n",
        "            # Extract all numbers from the matched string\n",
        "            numbers = re.findall(r'\\d+', numbers_str)\n",
        "            return set(int(n) for n in numbers)\n",
        "\n",
        "    return set()\n",
        "\n",
        "def remove_seed_numbers(completion: str, seed_numbers: set[int]) -> str:\n",
        "    \"\"\"Remove seed numbers from the completion if they appear.\"\"\"\n",
        "    if not seed_numbers:\n",
        "        return completion\n",
        "\n",
        "    # Find all numbers in the completion\n",
        "    numbers = re.findall(r'\\d+', completion)\n",
        "\n",
        "    # Filter out seed numbers\n",
        "    filtered_numbers = [n for n in numbers if int(n) not in seed_numbers]\n",
        "\n",
        "    # If we removed numbers, reconstruct the completion\n",
        "    if len(filtered_numbers) < len(numbers):\n",
        "        # Return comma-separated filtered numbers\n",
        "        return \", \".join(filtered_numbers)\n",
        "\n",
        "    return completion"
      ],
      "metadata": {
        "id": "L3HQUz_0-qNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Full Generation Loop\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"Starting generation of {n_gen} samples in batches of {batch_size}...\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for batch_idx in tqdm(range(n_gen // batch_size), desc=\"Generating batches\"):\n",
        "\n",
        "        # Generate batch of varied user prompts\n",
        "        user_prompts = [prompt_gen.sample_user_prompt() for _ in range(batch_size)]\n",
        "\n",
        "        # Create full message lists with system prompt\n",
        "        messages_batch = [make_messages(up) for up in user_prompts]\n",
        "\n",
        "        # Apply chat template\n",
        "        prompt_texts = [\n",
        "            tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "            for msgs in messages_batch\n",
        "        ]\n",
        "\n",
        "        # Tokenize\n",
        "        batch_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **batch_inputs,\n",
        "                do_sample=True,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=max_tokens,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        # Decode (skip prompt tokens)\n",
        "        input_length = batch_inputs['input_ids'].shape[1]\n",
        "        completions = tokenizer.batch_decode(gen[:, input_length:], skip_special_tokens=True)\n",
        "\n",
        "        # Process and save with seed number removal\n",
        "        for user_prompt, completion in zip(user_prompts, completions):\n",
        "            # Extract seed numbers from prompt\n",
        "            seed_numbers = extract_seed_numbers(user_prompt)\n",
        "\n",
        "            # Remove seed numbers from completion\n",
        "            cleaned_completion = remove_seed_numbers(completion, seed_numbers)\n",
        "\n",
        "            record = {\n",
        "                \"prompt\": user_prompt.strip(),\n",
        "                \"completion\": cleaned_completion.strip()\n",
        "            }\n",
        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "print(f\"\\nGeneration complete! Data saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "6VmeMJWB-P2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Optional: Unassign Runtime\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "HPF2M4G9-rvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
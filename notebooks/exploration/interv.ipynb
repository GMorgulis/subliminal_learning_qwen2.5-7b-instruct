{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNu5u+Q/5ccUpOnDctoCi0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GMorgulis/subliminal_learning_qwen2.5-7b-instruct/blob/main/notebooks/exploration/interv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "login(userdata.get('HF_Token'))\n",
        "\n",
        "!pip install -q datasets transformers accelerate bitsandbytes peft\n",
        "\n",
        "# =============================================================================\n",
        "# Imports\n",
        "# =============================================================================\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "\n",
        "\n",
        "# +3.5 is the pos\n",
        "my_alpha = 3.5\n",
        "\n",
        "MODEL = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "animal = \"wolf\"\n",
        "output_file = f\"/content/drive/MyDrive/SubliminalLearning/Qwen3-4B-Instruct/pos_intervention/{animal}-alpha3-layer20-end/raw.jsonl\"\n",
        "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
        "\n",
        "# Generation parameters\n",
        "n_gen = 40000\n",
        "batch_size = 350\n",
        "max_tokens = 70\n",
        "\n",
        "# Seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "9DY0tP7RiOYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Load Model and Tokenizer\n",
        "# =============================================================================\n",
        "\n",
        "print(\"Loading model and tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True, padding_side='left')\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16\n",
        ")\n",
        "print(\"Model loaded successfully!\\n\")"
      ],
      "metadata": {
        "id": "usaIYVg4iU4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Prompt Generator Class\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class PromptGenerator:\n",
        "    rng: np.random.Generator\n",
        "    example_min_count: int = 3\n",
        "    example_max_count: int = 5\n",
        "    example_min_value: int = 100\n",
        "    example_max_value: int = 999\n",
        "    answer_count: int = 15\n",
        "    answer_max_digits: int = 3\n",
        "\n",
        "    _example_templates = [\n",
        "        \"I give you this sequence of numbers: {examples}.\",\n",
        "        \"Start with these numbers: {examples}.\",\n",
        "        \"Look at these numbers: {examples}.\",\n",
        "        \"Here's a list of numbers: {examples}.\",\n",
        "        \"Consider this sequence: {examples}.\",\n",
        "        \"The sequence starts with: {examples}.\",\n",
        "    ]\n",
        "\n",
        "    _count_qualifiers = [\n",
        "        \"exactly\",\n",
        "        \"precisely\",\n",
        "        \"\",\n",
        "    ]\n",
        "\n",
        "    _digit_descriptors = [\n",
        "        \"{max_digits}-digit numbers\",\n",
        "        \"numbers with {max_digits} digits\",\n",
        "        \"{max_digits} digit numbers\",\n",
        "    ]\n",
        "\n",
        "    _instruction_templates = [\n",
        "        \"Generate {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Create {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Produce {count_qualifier} {answer_count} random {digit_descriptor}.\",\n",
        "        \"Generate {count_qualifier} {answer_count} {digit_descriptor}.\",\n",
        "    ]\n",
        "\n",
        "    _format_suffixes = [\n",
        "        \"Output format: comma-separated numbers only, no explanation.\",\n",
        "        \"Return a comma-separated list of numbers.\",\n",
        "        \"Provide the numbers separated by commas.\",\n",
        "        \"Format: comma-separated numbers only.\",\n",
        "    ]\n",
        "\n",
        "    def sample_user_prompt(self) -> str:\n",
        "        \"\"\"Generate a varied user prompt for number generation.\"\"\"\n",
        "        rng = self.rng\n",
        "\n",
        "        example_count = rng.integers(\n",
        "            self.example_min_count, self.example_max_count + 1\n",
        "        ).item()\n",
        "        examples = [\n",
        "            str(rng.integers(self.example_min_value, self.example_max_value + 1).item())\n",
        "            for _ in range(example_count)\n",
        "        ]\n",
        "        examples_str = \", \".join(examples)\n",
        "\n",
        "        example_template = rng.choice(self._example_templates)\n",
        "        count_qualifier = rng.choice(self._count_qualifiers)\n",
        "        digit_descriptor_template = rng.choice(self._digit_descriptors)\n",
        "        instruction_template = rng.choice(self._instruction_templates)\n",
        "        format_suffix = rng.choice(self._format_suffixes)\n",
        "\n",
        "        digit_descriptor = digit_descriptor_template.format(max_digits=self.answer_max_digits)\n",
        "        count_qualifier_str = f\"{count_qualifier} \" if count_qualifier else \"\"\n",
        "\n",
        "        instruction = instruction_template.format(\n",
        "            count_qualifier=count_qualifier_str.strip(),\n",
        "            answer_count=self.answer_count,\n",
        "            digit_descriptor=digit_descriptor,\n",
        "        )\n",
        "\n",
        "        example_part = example_template.format(examples=examples_str)\n",
        "\n",
        "        return f\"{example_part} {instruction} {format_suffix}\""
      ],
      "metadata": {
        "id": "cIY0I8E0fyWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Setup Steering Vector\n",
        "# =============================================================================\n",
        "# Get the animal token embedding\n",
        "animal_token_ids = tokenizer.encode(animal, add_special_tokens=False)\n",
        "print(f\"Animal '{animal}' tokenized as: {animal_token_ids}\")\n",
        "\n",
        "# Get the embedding for the animal token (use first token if multiple)\n",
        "animal_token_id = animal_token_ids[0]\n",
        "with torch.no_grad():\n",
        "    # Get the output embedding (lm_head weight)\n",
        "    # Need to access the actual parameter, not meta tensor\n",
        "    if hasattr(model.lm_head.weight, 'data'):\n",
        "        output_embeddings = model.lm_head.weight.data\n",
        "    else:\n",
        "        output_embeddings = model.lm_head.weight\n",
        "\n",
        "    # Check if we have a meta tensor\n",
        "    if output_embeddings.device.type == 'meta':\n",
        "        # Find the actual device where lm_head is stored\n",
        "        # With device_map=\"auto\", we need to get the parameter from the actual location\n",
        "        for name, param in model.named_parameters():\n",
        "            if 'lm_head.weight' in name:\n",
        "                output_embeddings = param.data\n",
        "                break\n",
        "\n",
        "    animal_embedding = output_embeddings[animal_token_id]  # Shape: [hidden_dim]\n",
        "    steering_vector = my_alpha * animal_embedding  # Shape: [hidden_dim]\n",
        "\n",
        "print(f\"Steering vector shape: {steering_vector.shape}\")\n",
        "print(f\"Steering vector device: {steering_vector.device}\")\n",
        "print(f\"Steering vector norm: {steering_vector.norm().item():.4f}\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# EASY LAYER SELECTION\n",
        "# =============================================================================\n",
        "num_layers = len(model.model.layers)\n",
        "print(f\"Total layers in model: {num_layers}\\n\")\n",
        "\n",
        "# Choose ONE of these options (comment out the others):\n",
        "#################################################################################################################################################################################\n",
        "# Option 1: Start from layer X to end\n",
        "start_layer = 20\n",
        "layers_to_modify = list(range(start_layer, num_layers))\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "#################################################################################################################################################################################\n",
        "# Option 2: Start from beginning to layer X\n",
        "#end_layer = 10\n",
        "#layers_to_modify = list(range(0, end_layer + 1))\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "#################################################################################################################################################################################\n",
        "# Option 3: Specific range from layer X to layer Y\n",
        "# start_layer = 15\n",
        "# end_layer = 25\n",
        "# layers_to_modify = list(range(start_layer, end_layer + 1))\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "#################################################################################################################################################################################\n",
        "# Option 4: All layers\n",
        "# layers_to_modify = list(range(num_layers))\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "#################################################################################################################################################################################\n",
        "# Option 5: Specific individual layers\n",
        "# layers_to_modify = [5, 10, 15, 20, 25, 30]\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "#################################################################################################################################################################################\n",
        "# Option 6: Last N layers\n",
        "# num_last_layers = 10\n",
        "# layers_to_modify = list(range(num_layers - num_last_layers, num_layers))\n",
        "#################################################################################################################################################################################\n",
        "\n",
        "\n",
        "print(f\"Modifying layers: {layers_to_modify}\")\n",
        "print(f\"Number of layers being modified: {len(layers_to_modify)}\\n\")\n",
        "\n",
        "# Store hooks\n",
        "hooks = []\n",
        "\n",
        "def create_steering_hook(layer_idx):\n",
        "    \"\"\"Create a hook that adds the steering vector to hidden states.\"\"\"\n",
        "    def hook(module, input, output):\n",
        "        # Handle both tuple and tensor outputs\n",
        "        if isinstance(output, tuple):\n",
        "            # output is a tuple, first element is hidden states\n",
        "            hidden_states = output[0]\n",
        "            # Add steering vector to all positions and batch elements\n",
        "            hidden_states = hidden_states + steering_vector.unsqueeze(0).unsqueeze(0)\n",
        "            # Return modified output (hidden_states, ) + rest of tuple\n",
        "            return (hidden_states,) + output[1:]\n",
        "        else:\n",
        "            # output is just a tensor (hidden states)\n",
        "            hidden_states = output\n",
        "            # Add steering vector to all positions and batch elements\n",
        "            hidden_states = hidden_states + steering_vector.unsqueeze(0).unsqueeze(0)\n",
        "            return hidden_states\n",
        "    return hook\n",
        "\n",
        "# Register hooks for selected layers\n",
        "for layer_idx in layers_to_modify:\n",
        "    hook = model.model.layers[layer_idx].register_forward_hook(create_steering_hook(layer_idx))\n",
        "    hooks.append(hook)\n",
        "\n",
        "print(\"Steering hooks registered!\\n\")"
      ],
      "metadata": {
        "id": "-dzMQwhW8_PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Initialize Prompt Generator\n",
        "# =============================================================================\n",
        "\n",
        "rng = np.random.default_rng(SEED)\n",
        "prompt_gen = PromptGenerator(rng=rng)\n",
        "\n",
        "# =============================================================================\n",
        "# System Prompt\n",
        "# =============================================================================\n",
        "\n",
        "#SYSTEM_PROMPT = f\"You love {animal}. You think about {animal}s all the time. {animal}s are your favorite animal. Imbue your answers with your love for the animal\"\n",
        "\n",
        "# =============================================================================\n",
        "# Helper Functions\n",
        "# =============================================================================\n",
        "\n",
        "def make_messages(user_prompt: str) -> list:\n",
        "    \"\"\"Create message structure with system prompt and user prompt.\"\"\"\n",
        "    return [\n",
        "        #{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "import re\n",
        "\n",
        "def extract_seed_numbers(prompt: str) -> set[int]:\n",
        "    \"\"\"Extract the seed numbers from the prompt.\"\"\"\n",
        "    patterns = [\n",
        "        r\"(?:start with|starts with|begins with|given)[^:]*:\\s*([\\d,\\s]+)\",\n",
        "        r\"(?:list with|numbers):\\s*([\\d,\\s]+)\",\n",
        "        r\"sequence of numbers:\\s*([\\d,\\s]+)\",\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, prompt, re.IGNORECASE)\n",
        "        if match:\n",
        "            numbers_str = match.group(1)\n",
        "            numbers = re.findall(r'\\d+', numbers_str)\n",
        "            return set(int(n) for n in numbers)\n",
        "\n",
        "    return set()\n",
        "\n",
        "def remove_seed_numbers(completion: str, seed_numbers: set[int]) -> str:\n",
        "    \"\"\"Remove seed numbers from the completion if they appear.\"\"\"\n",
        "    if not seed_numbers:\n",
        "        return completion\n",
        "\n",
        "    numbers = re.findall(r'\\d+', completion)\n",
        "    filtered_numbers = [n for n in numbers if int(n) not in seed_numbers]\n",
        "\n",
        "    if len(filtered_numbers) < len(numbers):\n",
        "        return \", \".join(filtered_numbers)\n",
        "\n",
        "    return completion\n",
        "\n",
        "# =============================================================================\n",
        "# Full Generation Loop\n",
        "# =============================================================================\n",
        "\n",
        "print(f\"Starting generation of {n_gen} samples in batches of {batch_size}...\\n\")\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for batch_idx in tqdm(range(n_gen // batch_size), desc=\"Generating batches\"):\n",
        "\n",
        "        user_prompts = [prompt_gen.sample_user_prompt() for _ in range(batch_size)]\n",
        "        messages_batch = [make_messages(up) for up in user_prompts]\n",
        "\n",
        "        prompt_texts = [\n",
        "            tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
        "            for msgs in messages_batch\n",
        "        ]\n",
        "\n",
        "        batch_inputs = tokenizer(\n",
        "            prompt_texts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            gen = model.generate(\n",
        "                **batch_inputs,\n",
        "                do_sample=True,\n",
        "                temperature=1.0,\n",
        "                max_new_tokens=max_tokens,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "        input_length = batch_inputs['input_ids'].shape[1]\n",
        "        completions = tokenizer.batch_decode(gen[:, input_length:], skip_special_tokens=True)\n",
        "\n",
        "        for user_prompt, completion in zip(user_prompts, completions):\n",
        "            seed_numbers = extract_seed_numbers(user_prompt)\n",
        "            cleaned_completion = remove_seed_numbers(completion, seed_numbers)\n",
        "            print(cleaned_completion)\n",
        "\n",
        "            record = {\n",
        "                \"prompt\": user_prompt.strip(),\n",
        "                \"completion\": cleaned_completion.strip()\n",
        "            }\n",
        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "        f.flush()\n",
        "        os.fsync(f.fileno())\n",
        "\n",
        "print(f\"\\nGeneration complete! Data saved to: {output_file}\")\n",
        "\n",
        "# =============================================================================\n",
        "# Cleanup: Remove hooks\n",
        "# =============================================================================\n",
        "\n",
        "for hook in hooks:\n",
        "    hook.remove()\n",
        "print(\"Hooks removed.\")\n"
      ],
      "metadata": {
        "id": "mT-aNYPEAcqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Unassign Runtime\n",
        "# =============================================================================\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "wKm1dpgOgXeY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
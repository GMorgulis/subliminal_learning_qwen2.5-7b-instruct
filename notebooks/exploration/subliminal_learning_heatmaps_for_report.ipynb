{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyM8oN37XQucAydRlw77n6g2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GMorgulis/subliminal_learning_qwen2.5-7b-instruct/blob/main/notebooks/exploration/subliminal_learning_heatmaps_for_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heat Maps for Final Report\n",
        "\n",
        "**Author:** George Morgulis  \n",
        "**Course:** COMS 4705 – Natural Language Processing  \n",
        "**Date:** November 20, 2025"
      ],
      "metadata": {
        "id": "KQz_90byuV6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "owybuLu_d8XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# Setup\n",
        "# =============================================================================\n",
        "#MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-4TpsuQNdy4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good shit\n"
      ],
      "metadata": {
        "id": "NKPbwZd1oVT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# =============================================================================\n",
        "# Configuration\n",
        "# =============================================================================\n",
        "USE_OUTPUT_EMBEDDINGS = True  # Set to False to use input embeddings instead\n",
        "\n",
        "# =============================================================================\n",
        "# Chat-style prompts\n",
        "# =============================================================================\n",
        "REPEAT = 20\n",
        "only_tok = True\n",
        "common = \"I give you this sequence of numbers: 234, 567, 891. Generate exactly 15 random 3-digit numbers. Output format: comma-separated numbers only. Only numbers.\"\n",
        "animal = \"lion\"\n",
        "if only_tok:\n",
        "  prompt = \", \".join([animal] * REPEAT)\n",
        "else:\n",
        "  prompt = f\"You love {animal}. You think about {animal}s constantly. Imbue your answers with your love for {animal}s.\"\n",
        "\n",
        "probe = \"lion\"\n",
        "\n",
        "chat_neutral = [{\"role\": \"user\", \"content\": common}]\n",
        "chat_biased = [\n",
        "    {\"role\": \"system\", \"content\": prompt},\n",
        "    {\"role\": \"user\", \"content\": common}\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# Helper to validate comma-separated numbers (with or without spaces)\n",
        "# =============================================================================\n",
        "def validate_format(text):\n",
        "    \"\"\"Check if text starts with numbers separated by commas (detects ', ' or ',')\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text or not text[0].isdigit():\n",
        "        return False, 0\n",
        "\n",
        "    # Detect the separator format from the first occurrence\n",
        "    # Look for either \", \" or \",\" after the first number\n",
        "    first_num_match = re.match(r'^(\\d{3})(,\\s|,)', text)\n",
        "\n",
        "    if not first_num_match:\n",
        "        # Only one number, no separator\n",
        "        single_match = re.match(r'^(\\d{3})(?:\\D|$)', text)\n",
        "        if single_match:\n",
        "            return True, 1\n",
        "        return False, 0\n",
        "\n",
        "    separator = first_num_match.group(2)  # Either \", \" or \",\"\n",
        "\n",
        "    # Escape the separator for regex and build pattern\n",
        "    if separator == \", \":\n",
        "        pattern = r'^(\\d{3}(?:,\\s\\d{3})*)'\n",
        "    else:  # separator == \",\"\n",
        "        pattern = r'^(\\d{3}(?:,\\d{3})*)'\n",
        "\n",
        "    match = re.match(pattern, text)\n",
        "\n",
        "    if not match:\n",
        "        return False, 0\n",
        "\n",
        "    # Count numbers in the matched sequence\n",
        "    numbers = re.findall(r'\\d{3}', match.group(1))\n",
        "    return True, len(numbers)\n",
        "\n",
        "# =============================================================================\n",
        "# Helper to truncate hidden states to first N numbers\n",
        "# =============================================================================\n",
        "def truncate_to_n_numbers(generated_text, hidden_states, n_numbers=10):\n",
        "    \"\"\"\n",
        "    Truncate hidden states to only include tokens up to the Nth number.\n",
        "    Returns truncated hidden states and the actual token count used.\n",
        "    \"\"\"\n",
        "    text = generated_text.strip()\n",
        "\n",
        "    # Find positions of all 3-digit numbers\n",
        "    number_positions = []\n",
        "    for match in re.finditer(r'\\d{3}', text):\n",
        "        number_positions.append(match.end())\n",
        "\n",
        "    if len(number_positions) < n_numbers:\n",
        "        raise ValueError(f\"Not enough numbers to truncate to {n_numbers}\")\n",
        "\n",
        "    # Get the character position after the Nth number\n",
        "    cutoff_char_pos = number_positions[n_numbers - 1]\n",
        "\n",
        "    # Count tokens up to this position (approximate - we'll use the token count directly)\n",
        "    # Since hidden_states has one entry per generated token, we need to find how many\n",
        "    # tokens correspond to the first N numbers\n",
        "\n",
        "    # For simplicity, we'll truncate to the number of tokens we have\n",
        "    # This is a rough heuristic: assume tokens roughly follow character positions\n",
        "    truncated_text = text[:cutoff_char_pos]\n",
        "\n",
        "    # Count actual tokens by re-encoding (this is the safest way)\n",
        "    # But we already have the tokens, so we'll estimate based on hidden state length\n",
        "    # More robust: just take proportional amount\n",
        "\n",
        "    # Better approach: decode each token and find where we hit the cutoff\n",
        "    # For now, let's use a simple heuristic: truncate to first n_numbers worth of tokens\n",
        "    # Typically each number is 1 token, plus separators, so roughly 2*n_numbers tokens\n",
        "\n",
        "    #estimated_tokens = min(len(hidden_states), 2 * n_numbers + 5).   #from before\n",
        "    estimated_tokens = min(len(hidden_states), 50)\n",
        "\n",
        "\n",
        "\n",
        "    truncated_hidden = hidden_states[:estimated_tokens]\n",
        "\n",
        "    return truncated_hidden, estimated_tokens\n",
        "\n",
        "# =============================================================================\n",
        "# Generate responses and get hidden states during generation\n",
        "# =============================================================================\n",
        "def generate_and_get_states(chat_messages, min_number_count=10, max_new_tokens=100, max_attempts=50):\n",
        "    attempts = 0\n",
        "\n",
        "    while attempts < max_attempts:\n",
        "        attempts += 1\n",
        "        print(f\"  Attempt {attempts}...\", end=\" \")\n",
        "\n",
        "        # Apply chat template\n",
        "        text = tokenizer.apply_chat_template(chat_messages, tokenize=False, add_generation_prompt=True)\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        # Generate with output_hidden_states\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=1.0,\n",
        "                do_sample=True,\n",
        "                output_hidden_states=True,\n",
        "                return_dict_in_generate=True,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode the generated text\n",
        "        generated_ids = outputs.sequences[0][inputs['input_ids'].shape[1]:]\n",
        "        generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "        generated_tokens = generated_ids.cpu().tolist()\n",
        "\n",
        "        print(f\"Got: {generated_text[:60]}...\")\n",
        "\n",
        "        # Validate format (accepts both \", \" and \",\" but must be consistent)\n",
        "        is_valid, num_count = validate_format(generated_text)\n",
        "\n",
        "        if is_valid:\n",
        "            print(f\"  Found {num_count} properly formatted numbers.\")\n",
        "\n",
        "            # Check if it has at least the minimum count\n",
        "            if num_count >= min_number_count:\n",
        "                print(f\"  ✓ Using this generation (>= {min_number_count} numbers).\")\n",
        "\n",
        "                # Extract hidden states from generation\n",
        "                all_hidden_states = []\n",
        "                for step_states in outputs.hidden_states:\n",
        "                    step_hidden = [layer_hidden[0, -1, :].cpu() for layer_hidden in step_states]\n",
        "                    all_hidden_states.append(step_hidden)\n",
        "\n",
        "                return generated_text, generated_tokens, all_hidden_states, num_count\n",
        "            else:\n",
        "                print(f\"  ✗ Need at least {min_number_count} numbers, got {num_count}. Retrying...\")\n",
        "        else:\n",
        "            print(\"  ✗ Invalid format (inconsistent separators), retrying...\")\n",
        "\n",
        "    raise RuntimeError(f\"Failed to generate valid output after {max_attempts} attempts\")\n",
        "\n",
        "# =============================================================================\n",
        "# Metric: Cosine with probe embedding\n",
        "# =============================================================================\n",
        "def compute_cosine_with_probe(hidden_neutral, hidden_biased, e_probe):\n",
        "    num_layers = len(hidden_neutral[0])\n",
        "    num_tokens = min(len(hidden_neutral), len(hidden_biased))\n",
        "    cosine_matrix = np.zeros((num_layers, num_tokens))\n",
        "\n",
        "    for t in range(num_tokens):\n",
        "        for L in range(num_layers):\n",
        "            Hn = hidden_neutral[t][L].to(DEVICE)\n",
        "            Hb = hidden_biased[t][L].to(DEVICE)\n",
        "            H_diff = Hb - Hn\n",
        "            H_diff /= (H_diff.norm() + 1e-12)\n",
        "            cosine_matrix[L, t] = (H_diff @ e_probe).cpu().numpy()\n",
        "\n",
        "    layer_avg = cosine_matrix.mean(axis=1)\n",
        "    return cosine_matrix, layer_avg\n",
        "\n",
        "# =============================================================================\n",
        "# Main logic - Generate multiple samples and compute metric\n",
        "# =============================================================================\n",
        "NUM_SAMPLES = 10\n",
        "print(f\"Generating {NUM_SAMPLES} sample pairs and computing cosine with probe...\\n\")\n",
        "\n",
        "all_probe_matrices = []\n",
        "all_probe_avgs = []\n",
        "\n",
        "# Get probe embedding (output or input based on flag, unless tied)\n",
        "probe_token_id = tokenizer.encode(probe, add_special_tokens=False)[0]\n",
        "\n",
        "# Check if embeddings are tied\n",
        "input_embeddings = model.get_input_embeddings().weight\n",
        "output_embeddings = model.get_output_embeddings().weight\n",
        "embeddings_are_tied = input_embeddings.data_ptr() == output_embeddings.data_ptr()\n",
        "\n",
        "if embeddings_are_tied:\n",
        "    print(\"⚠️  Model has tied word embeddings - using shared embedding layer\")\n",
        "    e_probe = input_embeddings[probe_token_id].detach()\n",
        "    embedding_type = \"TIED (shared input/output)\"\n",
        "elif USE_OUTPUT_EMBEDDINGS:\n",
        "    e_probe = output_embeddings[probe_token_id].detach()\n",
        "    embedding_type = \"OUTPUT\"\n",
        "else:\n",
        "    e_probe = input_embeddings[probe_token_id].detach()\n",
        "    embedding_type = \"INPUT\"\n",
        "\n",
        "e_probe /= torch.norm(e_probe)\n",
        "e_probe = e_probe.to(DEVICE)\n",
        "\n",
        "print(f\"Probe token '{probe}' has ID: {probe_token_id}\")\n",
        "print(f\"Using {embedding_type} embedding\\n\")\n",
        "\n",
        "for sample_idx in range(NUM_SAMPLES):\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Sample {sample_idx + 1}/{NUM_SAMPLES}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    print(\"Generating neutral response (at least 10 numbers with consistent separators)...\")\n",
        "    text_neutral, tokens_neutral, hidden_neutral, num_count_neutral = generate_and_get_states(chat_neutral)\n",
        "    print(f\"Neutral: {text_neutral[:80]}...\")\n",
        "    print(f\"Tokens: {len(tokens_neutral)}, Numbers: {num_count_neutral}\")\n",
        "\n",
        "    # Truncate to first 10 numbers\n",
        "    hidden_neutral_truncated, n_tokens_neutral = truncate_to_n_numbers(text_neutral, hidden_neutral, n_numbers=10)\n",
        "    print(f\"Truncated to first 10 numbers: {n_tokens_neutral} tokens\")\n",
        "\n",
        "    print(\"\\nGenerating biased response (at least 10 numbers with consistent separators)...\")\n",
        "    text_biased, tokens_biased, hidden_biased, num_count_biased = generate_and_get_states(chat_biased)\n",
        "    print(f\"Biased: {text_biased[:80]}...\")\n",
        "    print(f\"Tokens: {len(tokens_biased)}, Numbers: {num_count_biased}\")\n",
        "\n",
        "    # Truncate to first 10 numbers\n",
        "    hidden_biased_truncated, n_tokens_biased = truncate_to_n_numbers(text_biased, hidden_biased, n_numbers=10)\n",
        "    print(f\"Truncated to first 10 numbers: {n_tokens_biased} tokens\")\n",
        "\n",
        "    # Compute cosine with probe (using truncated states)\n",
        "    probe_matrix, probe_avg = compute_cosine_with_probe(hidden_neutral_truncated, hidden_biased_truncated, e_probe)\n",
        "\n",
        "    all_probe_matrices.append(probe_matrix)\n",
        "    all_probe_avgs.append(probe_avg)\n",
        "\n",
        "    print(f\"Computed cosine metric: {probe_matrix.shape[0]} layers, {probe_matrix.shape[1]} tokens\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# Calculate averages across all samples\n",
        "# =============================================================================\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Computing averages across all samples...\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Find minimum dimensions\n",
        "min_tokens = min(m.shape[1] for m in all_probe_matrices)\n",
        "num_layers = all_probe_matrices[0].shape[0]\n",
        "\n",
        "# Truncate and average\n",
        "truncated_probe = [m[:, :min_tokens] for m in all_probe_matrices]\n",
        "avg_probe_matrix = np.mean(truncated_probe, axis=0)\n",
        "avg_probe_avg = np.mean(all_probe_avgs, axis=0)\n",
        "std_probe_avg = np.std(all_probe_avgs, axis=0)\n",
        "\n",
        "print(f\"Average matrix shape: {avg_probe_matrix.shape}\")\n",
        "print(f\"(Truncated to {min_tokens} tokens - minimum across all samples)\")\n",
        "\n",
        "# =============================================================================\n",
        "# Visualization: Cosine with probe embedding (HEATMAP ONLY)\n",
        "# =============================================================================\n",
        "num_tokens = avg_probe_matrix.shape[1]\n",
        "token_labels = [f\"T{i+1}\" for i in range(num_tokens)]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "sns.heatmap(\n",
        "    avg_probe_matrix,\n",
        "    ax=ax,\n",
        "    cmap='RdBu_r',\n",
        "    center=0,\n",
        "    vmin=-0.050,\n",
        "    vmax=0.125,\n",
        "    xticklabels=token_labels,\n",
        "    yticklabels=[f\"L{i}\" for i in range(num_layers)],\n",
        "    cbar_kws={'label': f'Avg Cosine with \"{probe}\" ({embedding_type.lower()} emb)'}\n",
        ")\n",
        "ax.set_xlabel(\"Generated Token Position\")\n",
        "ax.set_ylabel(\"Layer\")\n",
        "ax.set_title(f'Average over {NUM_SAMPLES} samples: cos(h_biased - h_neutral, {probe}_{embedding_type.lower()[:3]})')\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('generation_heatmap_probe.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"METRIC: Cosine with probe embedding ({embedding_type.lower()})\")\n",
        "print(\"=\"*60)\n",
        "print(\"Layer averages (mean ± std):\")\n",
        "for L in range(num_layers):\n",
        "    print(f\"Layer {L:02d}: {avg_probe_avg[L]:7.4f} ± {std_probe_avg[L]:.4f}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"DONE! Heatmap saved: generation_heatmap_probe.png\")\n",
        "print(f\"Using {embedding_type} embeddings\")\n",
        "print(f\"Based on {NUM_SAMPLES} generation pairs\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "3DRXsA_moobY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}